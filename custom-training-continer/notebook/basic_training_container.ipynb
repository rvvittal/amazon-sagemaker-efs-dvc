{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basic Custom Training Container</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build and use a basic custom Docker container for training with Amazon SageMaker. Reference documentation is available at https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining some variables like the current execution role, the ECR repository that we are going to use for pushing the custom Docker container and a default Amazon S3 bucket to be used by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716664005094\n",
      "us-west-2\n",
      "arn:aws:iam::716664005094:role/TeamRole\n",
      "sagemaker-us-west-2-716664005094\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "ecr_namespace = 'sagemaker-training-containers/'\n",
    "prefix = 'basic-training-container'\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(account_id)\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the Dockerfile which defines the statements for building our custom SageMaker training container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Part of the implementation of this container is based on the Amazon SageMaker Apache MXNet container.\u001b[39;49;00m\r\n",
      "\u001b[37m# https://github.com/aws/sagemaker-mxnet-container\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33mubuntu:16.04\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mLABEL\u001b[39;49;00m \u001b[31mmaintainer\u001b[39;49;00m=\u001b[33m\"Amazon AI\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Defining some variables used at build time to install Python3\u001b[39;49;00m\r\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mPYTHON\u001b[39;49;00m=python3\r\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mPYTHON_PIP\u001b[39;49;00m=python3-pip\r\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mPIP\u001b[39;49;00m=pip3\r\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mPYTHON_VERSION\u001b[39;49;00m=\u001b[34m3\u001b[39;49;00m.6.6\r\n",
      "\r\n",
      "\u001b[37m# Install some handful libraries like curl, wget, git, build-essential, zlib\u001b[39;49;00m\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update && apt-get install -y --no-install-recommends software-properties-common && \u001b[33m\\\u001b[39;49;00m\r\n",
      "    add-apt-repository ppa:deadsnakes/ppa -y && \u001b[33m\\\u001b[39;49;00m\r\n",
      "    apt-get update && apt-get install -y --no-install-recommends \u001b[33m\\\u001b[39;49;00m\r\n",
      "        build-essential \u001b[33m\\\u001b[39;49;00m\r\n",
      "        ca-certificates \u001b[33m\\\u001b[39;49;00m\r\n",
      "        curl \u001b[33m\\\u001b[39;49;00m\r\n",
      "        wget \u001b[33m\\\u001b[39;49;00m\r\n",
      "        git \u001b[33m\\\u001b[39;49;00m\r\n",
      "        libopencv-dev \u001b[33m\\\u001b[39;49;00m\r\n",
      "        openssh-client \u001b[33m\\\u001b[39;49;00m\r\n",
      "        openssh-server \u001b[33m\\\u001b[39;49;00m\r\n",
      "        vim \u001b[33m\\\u001b[39;49;00m\r\n",
      "        zlib1g-dev && \u001b[33m\\\u001b[39;49;00m\r\n",
      "    rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "\u001b[37m# Installing Python3\u001b[39;49;00m\r\n",
      "\u001b[34mRUN\u001b[39;49;00m wget https://www.python.org/ftp/python/\u001b[31m$PYTHON_VERSION\u001b[39;49;00m/Python-\u001b[31m$PYTHON_VERSION\u001b[39;49;00m.tgz && \u001b[33m\\\u001b[39;49;00m\r\n",
      "        tar -xvf Python-\u001b[31m$PYTHON_VERSION\u001b[39;49;00m.tgz && \u001b[36mcd\u001b[39;49;00m Python-\u001b[31m$PYTHON_VERSION\u001b[39;49;00m && \u001b[33m\\\u001b[39;49;00m\r\n",
      "        ./configure && make && make install && \u001b[33m\\\u001b[39;49;00m\r\n",
      "        apt-get update && apt-get install -y --no-install-recommends libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev && \u001b[33m\\\u001b[39;49;00m\r\n",
      "        make && make install && rm -rf ../Python-\u001b[31m$PYTHON_VERSION\u001b[39;49;00m* && \u001b[33m\\\u001b[39;49;00m\r\n",
      "        ln -s /usr/local/bin/pip3 /usr/bin/pip\r\n",
      "\r\n",
      "\u001b[37m# Upgrading pip and creating symbolic link for python3\u001b[39;49;00m\r\n",
      "\u001b[34mRUN\u001b[39;49;00m \u001b[33m${\u001b[39;49;00m\u001b[31mPIP\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --no-cache-dir install --upgrade pip\r\n",
      "\u001b[34mRUN\u001b[39;49;00m ln -s \u001b[34m$(\u001b[39;49;00mwhich \u001b[33m${\u001b[39;49;00m\u001b[31mPYTHON\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[34m)\u001b[39;49;00m /usr/local/bin/python\r\n",
      "\r\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Installing numpy, pandas, scikit-learn, scipy\u001b[39;49;00m\r\n",
      "\u001b[34mRUN\u001b[39;49;00m \u001b[33m${\u001b[39;49;00m\u001b[31mPIP\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m install --no-cache --upgrade \u001b[33m\\\u001b[39;49;00m\r\n",
      "        \u001b[31mnumpy\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.14.5 \u001b[33m\\\u001b[39;49;00m\r\n",
      "        \u001b[31mpandas\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.24.1 \u001b[33m\\\u001b[39;49;00m\r\n",
      "        scikit-learn==\u001b[34m0\u001b[39;49;00m.20.3 \u001b[33m\\\u001b[39;49;00m\r\n",
      "        \u001b[31mrequests\u001b[39;49;00m==\u001b[34m2\u001b[39;49;00m.21.0 \u001b[33m\\\u001b[39;49;00m\r\n",
      "        \u001b[31mscipy\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.2.1\r\n",
      "\r\n",
      "\u001b[37m# Setting some environment variables.\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONDONTWRITEBYTECODE\u001b[39;49;00m=\u001b[34m1\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "    \u001b[31mPYTHONUNBUFFERED\u001b[39;49;00m=\u001b[34m1\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "    \u001b[31mLD_LIBRARY_PATH\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mLD_LIBRARY_PATH\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m:/usr/local/lib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "    \u001b[31mPYTHONIOENCODING\u001b[39;49;00m=UTF-8 \u001b[33m\\\u001b[39;49;00m\r\n",
      "    \u001b[31mLANG\u001b[39;49;00m=C.UTF-8 \u001b[33m\\\u001b[39;49;00m\r\n",
      "    \u001b[31mLC_ALL\u001b[39;49;00m=C.UTF-8\r\n",
      "\r\n",
      "\u001b[34mCOPY\u001b[39;49;00m code/* /\r\n",
      "\r\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"python\"\u001b[39;49;00m, \u001b[33m\"main.py\"\u001b[39;49;00m]\r\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ../docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At high-level the Dockerfile specifies the following operations for building this container:\n",
    "<ul>\n",
    "    <li>Start from Ubuntu 16.04</li>\n",
    "    <li>Define some variables to be used at build time to install Python 3</li>\n",
    "    <li>Some handful libraries are installed with apt-get</li>\n",
    "    <li>We then install Python 3 and create a symbolic link</li>\n",
    "    <li>We install some Python libraries like numpy, pandas, ScikitLearn, etc.</li>\n",
    "    <li>We set e few environment variables, including PYTHONUNBUFFERED which is used to avoid buffering Python standard output (useful for logging)</li>\n",
    "    <li>Finally, we copy all contents in <strong>code/</strong> (which is where our training code is) to the WORKDIR and define the ENTRYPOINT</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build and push the container</h3>\n",
    "We are now ready to build this container and push it to Amazon ECR. This task is executed using a shell script stored in the ../script/ folder. Let's take a look at this script and then execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mACCOUNT_ID\u001b[39;49;00m=\u001b[31m$1\u001b[39;49;00m\r\n",
      "\u001b[31mREGION\u001b[39;49;00m=\u001b[31m$2\u001b[39;49;00m\r\n",
      "\u001b[31mREPO_NAME\u001b[39;49;00m=\u001b[31m$3\u001b[39;49;00m\r\n",
      "\r\n",
      "docker build -f ../docker/Dockerfile -t \u001b[31m$REPO_NAME\u001b[39;49;00m ../docker\r\n",
      "\r\n",
      "docker tag \u001b[31m$REPO_NAME\u001b[39;49;00m \u001b[31m$ACCOUNT_ID\u001b[39;49;00m.dkr.ecr.\u001b[31m$REGION\u001b[39;49;00m.amazonaws.com/\u001b[31m$REPO_NAME\u001b[39;49;00m:latest\r\n",
      "\r\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --no-include-email --registry-ids \u001b[31m$ACCOUNT_ID\u001b[39;49;00m\u001b[34m)\u001b[39;49;00m\r\n",
      "\r\n",
      "aws ecr describe-repositories --repository-names \u001b[31m$REPO_NAME\u001b[39;49;00m || aws ecr create-repository --repository-name \u001b[31m$REPO_NAME\u001b[39;49;00m\r\n",
      "\r\n",
      "docker push \u001b[31m$ACCOUNT_ID\u001b[39;49;00m.dkr.ecr.\u001b[31m$REGION\u001b[39;49;00m.amazonaws.com/\u001b[31m$REPO_NAME\u001b[39;49;00m:latest\r\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ../scripts/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>--------------------------------------------------------------------------------------------------------------------</h3>\n",
    "\n",
    "The script builds the Docker container, then creates the repository if it does not exist, and finally pushes the container to the ECR repository. The build task requires a few minutes to be executed the first time, then Docker caches build outputs to be reused for the subsequent build operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! ../scripts/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training with Amazon SageMaker</h3>\n",
    "\n",
    "Once we have correctly pushed our container to Amazon ECR, we are ready to start training with Amazon SageMaker, which requires the ECR path to the Docker container used for training as parameter for starting a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716664005094.dkr.ecr.us-west-2.amazonaws.com/sagemaker-training-containers/basic-training-container:latest\n"
     ]
    }
   ],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the purpose of this example is explaining how to build custom containers, we are not going to train a real model. The script that will be executed does not define a specific training logic; it just outputs the configurations injected by SageMaker and implements a dummy training loop. Training data is also dummy. Let's analyze the code first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m absolute_import\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ExitSignalHandler\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m write_failure_file, print_json_object, load_json_object, save_model_artifacts, print_files_in_path\r\n",
      "\r\n",
      "hyperparameters_file_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/config/hyperparameters.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "inputdataconfig_file_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/config/inputdataconfig.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "resource_file_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/config/resourceconfig.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "data_files_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "failure_file_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/output/failure\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "model_artifacts_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "training_job_name_env = \u001b[33m\"\u001b[39;49;00m\u001b[33mTRAINING_JOB_NAME\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "training_job_arn_env = \u001b[33m\"\u001b[39;49;00m\u001b[33mTRAINING_JOB_ARN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m():\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mRunning training...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[34mif\u001b[39;49;00m os.path.exists(hyperparameters_file_path):\r\n",
      "            hyperparameters = load_json_object(hyperparameters_file_path)\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mHyperparameters configuration:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            print_json_object(hyperparameters)\r\n",
      "        \r\n",
      "        \u001b[34mif\u001b[39;49;00m os.path.exists(inputdataconfig_file_path):\r\n",
      "            input_data_config = load_json_object(inputdataconfig_file_path)\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mInput data configuration:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            print_json_object(input_data_config)\r\n",
      "            \r\n",
      "            \u001b[34mfor\u001b[39;49;00m key \u001b[35min\u001b[39;49;00m input_data_config:\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mList of files in \u001b[39;49;00m\u001b[33m{0}\u001b[39;49;00m\u001b[33m channel: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(key))\r\n",
      "                channel_path = data_files_path + key + \u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                print_files_in_path(channel_path)\r\n",
      "        \r\n",
      "        \u001b[34mif\u001b[39;49;00m os.path.exists(resource_file_path):\r\n",
      "            resource_config = load_json_object(resource_file_path)\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mResource configuration:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            print_json_object(resource_config)\r\n",
      "            \r\n",
      "        \u001b[34mif\u001b[39;49;00m (training_job_name_env \u001b[35min\u001b[39;49;00m os.environ):\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTraining job name: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(os.environ[training_job_name_env])\r\n",
      "        \r\n",
      "        \u001b[34mif\u001b[39;49;00m (training_job_arn_env \u001b[35min\u001b[39;49;00m os.environ):\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTraining job ARN: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(os.environ[training_job_arn_env])\r\n",
      "            \r\n",
      "        \u001b[37m# This object is used to handle SIGTERM and SIGKILL signals.\u001b[39;49;00m\r\n",
      "        signal_handler = ExitSignalHandler()\r\n",
      "        \r\n",
      "        \u001b[37m# Dummy net.\u001b[39;49;00m\r\n",
      "        net = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# Run training loop.\u001b[39;49;00m\r\n",
      "        epochs = \u001b[34m5\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(epochs):\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mRunning epoch \u001b[39;49;00m\u001b[33m{0}\u001b[39;49;00m\u001b[33m...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(x))\r\n",
      "\r\n",
      "            time.sleep(\u001b[34m2\u001b[39;49;00m)\r\n",
      "            \r\n",
      "            \u001b[34mif\u001b[39;49;00m (signal_handler.exit_now):\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReceived SIGTERM/SIGINT. Saving training state and exiting.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "                \u001b[37m# Save state here.\u001b[39;49;00m\r\n",
      "                save_model_artifacts(model_artifacts_path, net)\r\n",
      "                sys.exit(\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCompleted epoch \u001b[39;49;00m\u001b[33m{0}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(x))\r\n",
      "        \r\n",
      "        \u001b[37m# At the end of the training loop, we have to save model artifacts.\u001b[39;49;00m\r\n",
      "        save_model_artifacts(model_artifacts_path, net)\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTraining completed!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "        write_failure_file(failure_file_path, \u001b[36mstr\u001b[39;49;00m(e))\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(e, file=sys.stderr)\r\n",
      "        sys.exit(\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    \u001b[34mif\u001b[39;49;00m (sys.argv[\u001b[34m1\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\r\n",
      "        train()\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMissing required argument \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, file=sys.stderr)\r\n",
      "        sys.exit(\u001b[34m1\u001b[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ../docker/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload some dummy data to Amazon S3, in order to define our S3-based training channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-716664005094/basic-training-container/train/dummy.csv\n",
      "s3://sagemaker-us-west-2-716664005094/basic-training-container/val/dummy.csv\n"
     ]
    }
   ],
   "source": [
    "#! echo \"val1, val2, val3\" > dummy.csv\n",
    "#print(sagemaker_session.upload_data('dummy.csv', bucket, prefix + '/train'))\n",
    "#print(sagemaker_session.upload_data('dummy.csv', bucket, prefix + '/val'))\n",
    "#! rm dummy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can execute the training job by calling the fit() method of the generic Estimator object defined in the Amazon SageMaker Python SDK (https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/estimator.py). This corresponds to calling the CreateTrainingJob() API (https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-01 17:40:10 Starting - Starting the training job...\n",
      "2021-02-01 17:40:36 Starting - Launching requested ML instancesProfilerReport-1612201210: InProgress\n",
      ".........\n",
      "2021-02-01 17:41:57 Starting - Preparing the instances for training...\n",
      "2021-02-01 17:42:37 Downloading - Downloading input data...\n",
      "2021-02-01 17:42:58 Training - Downloading the training image...\n",
      "2021-02-01 17:43:38 Training - Training image download completed. Training in progress..\u001b[34mRunning training...\n",
      "\u001b[0m\n",
      "\u001b[34mHyperparameters configuration:\u001b[0m\n",
      "\u001b[34m{'hp1': 'value1', 'hp2': '300', 'hp3': '0.001'}\n",
      "\u001b[0m\n",
      "\u001b[34mInput data configuration:\u001b[0m\n",
      "\u001b[34m{'train': {'ContentType': 'image/jpeg',\n",
      "           'RecordWrapperType': 'None',\n",
      "           'S3DistributionType': 'FullyReplicated',\n",
      "           'TrainingInputMode': 'File'},\n",
      " 'validation': {'ContentType': 'image/jpeg',\n",
      "                'RecordWrapperType': 'None',\n",
      "                'S3DistributionType': 'FullyReplicated',\n",
      "                'TrainingInputMode': 'File'}}\n",
      "\u001b[0m\n",
      "\u001b[34mList of files in validation channel: \u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B10.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B9.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_BQA.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B6.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B10.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B1.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B9_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B11.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_BQA.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B3.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_thumb_small.jpg\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B8.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B2_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B5.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B7.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B8_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B7_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B6.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B4.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B1.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B1_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_thumb_large.jpg\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B10_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B8.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B6_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B2.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B4.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B7.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B2.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_MTL.txt\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B11.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B9.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B5.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_MTL.json\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_BQA_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B5_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B3.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/index.html\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B4_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B3_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016246LGN00_B11_wrk.IMD\n",
      "\u001b[0m\n",
      "\u001b[34mList of files in train channel: \u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B5.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B5_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B9.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B5.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B10.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_BQA_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B4.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B6.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B11.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B7.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B1.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B9_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B8.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B3.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B11.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B2.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_BQA.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B10_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B8_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B11_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_BQA.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B6.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_MTL.json\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_thumb_large.jpg\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_MTL.txt\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B6_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B9.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B2_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B10.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B8.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B4.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B1_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B4_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B3_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B3.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B1.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B2.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_thumb_small.jpg\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/index.html\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B7_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B7.TIF.ovr\n",
      "\u001b[0m\n",
      "\u001b[34mResource configuration:\u001b[0m\n",
      "\u001b[34m{'current_host': 'algo-1',\n",
      " 'hosts': ['algo-1'],\n",
      " 'network_interface_name': 'eth0'}\n",
      "\u001b[0m\n",
      "\u001b[34mTraining job name: \u001b[0m\n",
      "\u001b[34mbasic-training-container-2021-02-01-17-40-10-166\n",
      "\u001b[0m\n",
      "\u001b[34mTraining job ARN: \u001b[0m\n",
      "\u001b[34marn:aws:sagemaker:us-west-2:716664005094:training-job/basic-training-container-2021-02-01-17-40-10-166\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 0...\u001b[0m\n",
      "\u001b[34mCompleted epoch 0.\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 1...\u001b[0m\n",
      "\u001b[34mCompleted epoch 1.\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 2...\u001b[0m\n",
      "\u001b[34mCompleted epoch 2.\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 3...\u001b[0m\n",
      "\u001b[34mCompleted epoch 3.\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 4...\u001b[0m\n",
      "\u001b[34mCompleted epoch 4.\n",
      "\u001b[0m\n",
      "\u001b[34mTraining completed!\u001b[0m\n",
      "\n",
      "2021-02-01 17:43:59 Uploading - Uploading generated training model\n",
      "2021-02-01 17:43:59 Completed - Training job completed\n",
      "Training seconds: 93\n",
      "Billable seconds: 93\n",
      "CPU times: user 694 ms, sys: 26.2 ms, total: 720 ms\n",
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "est = sagemaker.estimator.Estimator(container_image_uri,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    #train_instance_type='local', # use local mode\n",
    "                                    train_instance_type='ml.m5.xlarge',\n",
    "                                    base_job_name=prefix)\n",
    "\n",
    "est.set_hyperparameters(hp1='value1',\n",
    "                        hp2=300,\n",
    "                        hp3=0.001)\n",
    "\n",
    "train_config = sagemaker.session.s3_input('s3://landsat-pds/L8/001/002/LC80010022016230LGN00/', content_type='image/jpeg')\n",
    "val_config = sagemaker.session.s3_input('s3://landsat-pds/L8/001/002/LC80010022016246LGN00/', content_type='image/jpeg')\n",
    "\n",
    "est.fit({'train': train_config, 'validation': val_config })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: dkms-build-requires, priorities, update-motd, upgrade-helper,\n",
      "              : versionlock\n",
      "amzn-main                                                | 2.1 kB     00:00     \n",
      "amzn-updates                                             | 3.8 kB     00:00     \n",
      "copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxi | 3.0 kB     00:00     \n",
      "libnvidia-container/x86_64/signature                     |  833 B     00:00     \n",
      "libnvidia-container/x86_64/signature                     | 2.1 kB     00:00 !!! \n",
      "nvidia-container-runtime/x86_64/signature                |  833 B     00:00     \n",
      "nvidia-container-runtime/x86_64/signature                | 2.1 kB     00:00 !!! \n",
      "nvidia-docker/x86_64/signature                           |  488 B     00:00     \n",
      "nvidia-docker/x86_64/signature                           | 2.1 kB     00:00 !!! \n",
      "Package amazon-efs-utils-1.28.2-1.amzn1.noarch already installed and latest version\n",
      "Nothing to do\n"
     ]
    }
   ],
   "source": [
    "!sudo yum install -y amazon-efs-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir efs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user\r\n"
     ]
    }
   ],
   "source": [
    "!echo $HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo mount -t efs fs-df51afdb:/ $HOME/efs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp --recursive s3://landsat-pds/L8/001/002/ $HOME/efs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFS file-system-id: fs-df51afdb\n",
      "EFS file-system data input path: /LC80010022016230LGN00\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# Specify EFS ile system id.\n",
    "file_system_id = 'fs-df51afdb'\n",
    "print(f\"EFS file-system-id: {file_system_id}\")\n",
    "\n",
    "# Specify directory path for input data on the file system. \n",
    "# You need to provide normalized and absolute path below.\n",
    "file_system_directory_path = '/LC80010022016230LGN00'\n",
    "print(f'EFS file-system data input path: {file_system_directory_path}')\n",
    "\n",
    "# Specify the access mode of the mount of the directory associated with the file system. \n",
    "# Directory must be mounted  'ro'(read-only).\n",
    "file_system_access_mode = 'ro'\n",
    "\n",
    "# Specify your file system type\n",
    "file_system_type = 'EFS'\n",
    "\n",
    "train = FileSystemInput(file_system_id=file_system_id,\n",
    "                                    file_system_type=file_system_type,\n",
    "                                    directory_path=file_system_directory_path,\n",
    "                                    file_system_access_mode=file_system_access_mode)\n",
    "\n",
    "file_system_directory_path = '/LC80010022016246LGN00'\n",
    "\n",
    "validation = FileSystemInput(file_system_id=file_system_id,\n",
    "                                    file_system_type=file_system_type,\n",
    "                                    directory_path=file_system_directory_path,\n",
    "                                    file_system_access_mode=file_system_access_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-31 05:38:19 Starting - Starting the training job...\n",
      "2021-01-31 05:38:43 Starting - Launching requested ML instancesProfilerReport-1612071499: InProgress\n",
      "......\n",
      "2021-01-31 05:39:46 Starting - Preparing the instances for training......\n",
      "2021-01-31 05:40:44 Downloading - Downloading input data\n",
      "2021-01-31 05:40:44 Training - Downloading the training image...\n",
      "2021-01-31 05:41:19 Uploading - Uploading generated training model\u001b[34mRunning training...\n",
      "\u001b[0m\n",
      "\u001b[34mHyperparameters configuration:\u001b[0m\n",
      "\u001b[34m{'hp1': 'value1', 'hp2': '300', 'hp3': '0.001'}\n",
      "\u001b[0m\n",
      "\u001b[34mInput data configuration:\u001b[0m\n",
      "\u001b[34m{'train': {'RecordWrapperType': 'None',\n",
      "           'S3DistributionType': 'FullyReplicated',\n",
      "           'TrainingInputMode': 'File'},\n",
      " 'validation': {'RecordWrapperType': 'None',\n",
      "                'S3DistributionType': 'FullyReplicated',\n",
      "                'TrainingInputMode': 'File'}}\n",
      "\u001b[0m\n",
      "\u001b[34mList of files in validation channel: \u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B5.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B3_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B8.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B9.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B4.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B1.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B10.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B7_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_BQA.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B7.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B11.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B2.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B5_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B10_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B1_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B6.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B3.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B9_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B6.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_thumb_large.jpg\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B10.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B1.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B4_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B9.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B5.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B2.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B11.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_MTL.json\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B8_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_BQA.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B8.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_MTL.txt\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B3.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B6_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/index.html\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B11_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B2_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B7.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_BQA_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_thumb_small.jpg\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/LC80010022016230LGN00_B4.TIF\n",
      "\u001b[0m\n",
      "\u001b[34mList of files in train channel: \u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B5.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B3_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B8.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B9.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B4.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B1.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B10.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B7_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_BQA.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B7.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B11.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B2.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B5_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B10_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B1_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B6.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B3.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B9_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B6.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_thumb_large.jpg\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B10.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B1.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B4_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B9.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B5.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B2.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B11.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_MTL.json\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B8_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_BQA.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B8.TIF\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_MTL.txt\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B3.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B6_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/index.html\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B11_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B2_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B7.TIF.ovr\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_BQA_wrk.IMD\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_thumb_small.jpg\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/LC80010022016230LGN00_B4.TIF\n",
      "\u001b[0m\n",
      "\u001b[34mResource configuration:\u001b[0m\n",
      "\u001b[34m{'current_host': 'algo-1',\n",
      " 'hosts': ['algo-1'],\n",
      " 'network_interface_name': 'eth0'}\n",
      "\u001b[0m\n",
      "\u001b[34mTraining job name: \u001b[0m\n",
      "\u001b[34mbasic-training-container-2021-01-31-05-38-19-414\n",
      "\u001b[0m\n",
      "\u001b[34mTraining job ARN: \u001b[0m\n",
      "\u001b[34marn:aws:sagemaker:us-west-2:716664005094:training-job/basic-training-container-2021-01-31-05-38-19-414\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 0...\u001b[0m\n",
      "\u001b[34mCompleted epoch 0.\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 1...\u001b[0m\n",
      "\u001b[34mCompleted epoch 1.\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 2...\u001b[0m\n",
      "\u001b[34mCompleted epoch 2.\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 3...\u001b[0m\n",
      "\u001b[34mCompleted epoch 3.\n",
      "\u001b[0m\n",
      "\u001b[34mRunning epoch 4...\u001b[0m\n",
      "\u001b[34mCompleted epoch 4.\n",
      "\u001b[0m\n",
      "\u001b[34mTraining completed!\u001b[0m\n",
      "\n",
      "2021-01-31 05:41:45 Completed - Training job completed\n",
      "Training seconds: 64\n",
      "Billable seconds: 64\n",
      "CPU times: user 608 ms, sys: 25.9 ms, total: 634 ms\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "security_group_ids = ['sg-0c21b70b7f1480a4e']\n",
    "subnets = ['subnet-0370146357224ed30']\n",
    "\n",
    "est = sagemaker.estimator.Estimator(container_image_uri,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    #train_instance_type='local', # use local mode\n",
    "                                    train_instance_type='ml.m5.xlarge',\n",
    "                                    base_job_name=prefix,\n",
    "                                    subnets=subnets,\n",
    "                                    security_group_ids=security_group_ids)\n",
    "\n",
    "est.set_hyperparameters(hp1='value1',\n",
    "                        hp2=300,\n",
    "                        hp3=0.001)\n",
    "\n",
    "data_channels = {'train': train, 'validation': validation}\n",
    "\n",
    "est.fit(inputs=data_channels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
